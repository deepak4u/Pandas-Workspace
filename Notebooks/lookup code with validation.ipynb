{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f96232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "pr_ref_header_list =['Report Date', 'Country', 'Product Barcode', 'Flipkart ProductID', 'Flipkart ProductName', 'Category Level1', 'Category Level2',\n",
    "       'Category Level3', 'Category Level4', 'Site', 'Number Of offers', 'Sku Id', 'Brand', 'Product Name', 'Size', 'Color', 'Other Variants',\n",
    "       'Currency', 'Was_Price', 'Now Price', 'Discount', 'Shipping Cost', 'Product Availability', 'Stock Information', 'Seller Name',\n",
    "       'Buybox Flag', 'Seller Type', 'Min Price Flag', 'Limited_Stock_Info', 'Warranty', 'Warranty Period', 'Product Condition', 'Product InputUrl',\n",
    "       'Product Output Url', 'Timestamp', 'Extract Date', 'Category from site1', 'Category from site2', 'Category from site3',\n",
    "       'Category from site4', 'Category from site5', 'Category from site6', 'Fulfillment', 'Extra2', 'Extra3', 'Extra4']\n",
    "\n",
    "# Current Working Directory\n",
    "lookup_dir = os.getcwd()\n",
    "# Client mapping file df - Hardcoded path\n",
    "client_map_file_path = \"E:\\Flipkart_Price_Refresh\\Input_lookup_file\\price_v2_template_data_updated_SQL - All.xlsx\"\n",
    "df_client_map = pd.read_excel(client_map_file_path)\n",
    "# Input csv files path\n",
    "input_csv_lst = glob.glob(os.path.join(lookup_dir+'\\\\Input\\\\','*.csv'))\n",
    "\n",
    "def column_validation():\n",
    "    _ , final_column_count = df_final_output.shape\n",
    "    \n",
    "    if pr_ref_header_list != list(df_final_output.columns):\n",
    "        if final_column_count == 46:\n",
    "            return \"\\033[1m\" + \"\\033[91m\" + \"  |  \" + \"Warning: \" + \"Headers are different\"\n",
    "        else:\n",
    "            return \"\\033[1m\" + \"\\033[91m\" + \"  |  \" + \"Warning: \" + str(final_column_count) + \" Columns\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def row_validation():\n",
    "    before_lookup_row_count, _ = df.shape\n",
    "    final_row_count, _ = df_final_output.shape\n",
    "    missing_rows = before_lookup_row_count - final_row_count\n",
    "    \n",
    "    if before_lookup_row_count != final_row_count:\n",
    "        return \"\\033[1m\" + \"\\033[91m\" + \"  |  \" + \"Warning: \" + str(missing_rows) + \" rows missing after lookup\"\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "def date_validation():    \n",
    "    today = date.today()\n",
    "    dt_string = today.strftime('%Y-%m-%d')\n",
    "    \n",
    "    report_date_validate = df_final_output['Report Date'].str.contains(dt_string).all(skipna=True)\n",
    "    timestamp_validate = df_final_output['Timestamp'].str.contains(dt_string).all(skipna=True)\n",
    "    extract_date_validate = df_final_output['Extract Date'].str.contains(dt_string).all(skipna=True)\n",
    "    \n",
    "    if report_date_validate == False or timestamp_validate == False or extract_date_validate == False:\n",
    "        return \"\\033[1m\" + \"\\033[91m\" + \"  |  \" + \"Warning: \" + \"Date is different\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def comma_validation():\n",
    "    # Todo\n",
    "    if comma_replace:\n",
    "        return \"\\033[1m\" + \"\\033[91m\" + \"  |  \" + \"Warning: \" + \"Comma not replaced\"\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "# ----- Lookup Code -----\n",
    "\n",
    "url_status_count_list = []\n",
    "\n",
    "for file in input_csv_lst:\n",
    "    # Input csv file df\n",
    "    file_name_csv = file.split('\\\\')[-1].replace('.csv', '')\n",
    "    file_path_csv = file\n",
    "    df = pd.read_csv(file_path_csv)\n",
    "    \n",
    "    # Check and remove all NaN rows \n",
    "    df.dropna(axis=0, how='all', inplace=True)\n",
    "    \n",
    "    # merge, conversion, and deletion\n",
    "    df_merged = df.merge(df_client_map, on='Extra3')\n",
    "    \n",
    "    # Todo: Del\n",
    "    df_merged_test = df_merged.copy(deep=True)\n",
    "    \n",
    "    \n",
    "    int_conv_col_lst_var = ['Number Of offers', 'Stock Information', 'Buybox Flag', 'Min Price Flag', 'Product Barcode', 'Flipkart ProductID']\n",
    "    int_conv_col_lst_val = ['Number Of offers', 'Stock Information', 'Buybox Flag', 'Min Price Flag', 'item_barcode', ' Flipkart_Internal_ID ']\n",
    "    float_conv_col_lst_var = ['Now Price']\n",
    "    float_conv_col_lst_val = ['Now Price']\n",
    "    rename_col_lst_var = ['Flipkart ProductName', 'Category Level1', 'Category Level2', 'Category Level3', 'Category Level4']\n",
    "    rename_col_lst_val = [' Flipkart_product_online_name', 'Dept (Category_level_1)', 'Section (Category_level_2)', 'Family (Category_level_3)', 'Sub_Family(Category_level_4)']\n",
    "    del_col_name_lst = [' Flipkart_product_online_name', 'Competitor', 'competitor_url', 'item_barcode', ' Flipkart_Internal_ID ', 'input_B', 'Dept (Category_level_1)', 'Section (Category_level_2)', 'Family (Category_level_3)', 'Sub_Family(Category_level_4)', 'Competitor_Internal_product ID']\n",
    "    # 'Unnamed: 12' del -  removed from file\n",
    "    \n",
    "    # safely convert non-numeric type\n",
    "    #df_merged[int_conv_col_lst_var] = df_merged[int_conv_col_lst_val].apply(pd.to_numeric, errors='raise')\n",
    "    df_merged[int_conv_col_lst_var] = df_merged[int_conv_col_lst_val].apply(pd.to_numeric, errors='ignore')\n",
    "    df_merged[float_conv_col_lst_var] = df_merged[float_conv_col_lst_val].apply(pd.to_numeric, errors='raise')\n",
    "    \n",
    "    df_merged[rename_col_lst_var] = df_merged[rename_col_lst_val]\n",
    "    df_merged.drop(del_col_name_lst, axis=1, inplace=True)\n",
    "    #del df_merged[del_col_name_lst]\n",
    "    #df_merged['Product Barcode'] = df_merged['item_barcode'].apply(lambda p: barcode(p) if p == p else '')\n",
    "    \n",
    "    # Final Output Dataframe\n",
    "    df_final_output = df_merged\n",
    "    df_final_output.to_csv(lookup_dir + '\\\\Output\\\\' + file_name_csv + '.csv',index=False)\n",
    "    \n",
    "    # Url status count dict append and df conv\n",
    "    url_status_columns = ['file_name_csv', 'unique_urls', 'total_listings','Valid URLs', 'Blocked URLs', 'Invalid URLs(inactive)', 'Valid URLs OOS (Without price)', 'Script time out URLs']\n",
    "    url_status_count_dict = dict(df_final_output['Extra2'].value_counts())\n",
    "    url_status_count_dict['file_name_csv'] = file_name_csv\n",
    "    url_status_count_dict['unique_urls'] = df_final_output['Extra3'].nunique()\n",
    "    url_status_count_dict['total_listings'] = df_final_output.shape[0]\n",
    "    url_status_count_list.append(url_status_count_dict)\n",
    "    df_url_status = pd.DataFrame(url_status_count_list, columns=url_status_columns)\n",
    "    \n",
    "    # Validation function call\n",
    "    print(file_name_csv + str(df_final_output.shape) + column_validation() + row_validation() + date_validation(), end=\"\\n\\x1b[0m\")\n",
    "    \n",
    "print(\"\\n \\033[1m\" + \"\\033[92m\" + \"---------- Lookup Completed ----------\")\n",
    "\n",
    "# Summary data count\n",
    "df_url_status.iloc[:,1:] = df_url_status.iloc[:,1:].astype('Int64').astype(object).fillna('-')\n",
    "df_url_status.index+=1\n",
    "df_url_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d03402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2536a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb33c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a40c525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcdd13a4",
   "metadata": {},
   "source": [
    "### Old lookup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf02fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os,glob\n",
    "import math as ma\n",
    "def barcode(x):\n",
    "    if x == x:\n",
    "        try:\n",
    "            return int(x)\n",
    "        except:return x\n",
    "InputPath=os.getcwd()\n",
    "InputCSV=glob.glob(os.path.join(InputPath+'//Input_lookup_file//','*.xlsx'))\n",
    "for i in InputCSV:\n",
    "    InputDF=pd.read_excel(i)\n",
    "\n",
    "OutputPath=os.getcwd()\n",
    "OutputCSV=glob.glob(os.path.join(OutputPath+'//Input//','*.csv'))\n",
    "for f in OutputCSV:\n",
    "    OutputDF=pd.read_csv(f)\n",
    "    filename=f.split('\\\\')[-1].replace('.csv','')\n",
    "    FinalPath=os.getcwd()\n",
    "    file=OutputDF.merge(InputDF,on='Extra3')\n",
    "    # Souq column conversion to int - Change column format 0  in SKU ID before saving cvs file\n",
    "    # Uncomment Below line ['Sku Id'] for Souq Egypt\n",
    "#     file['Sku Id']=file['Sku Id'].apply(lambda t: int(t) if t == t else '')\n",
    "    file['Number Of offers']=file['Number Of offers'].apply(lambda a: int(a) if a == a else '')\n",
    "    file['Now Price']=file['Now Price'].apply(lambda a: float(a) if a == a else '')\n",
    "    file['Stock Information']=file['Stock Information'].apply(lambda b: int(b) if b == b else '')\n",
    "    file['Buybox Flag']=file['Buybox Flag'].apply(lambda y: int(y) if y == y else '')\n",
    "    file['Min Price Flag']=file['Min Price Flag'].apply(lambda z: int(z) if z == z else '')\n",
    "    file['Flipkart ProductName']=file[' Flipkart_product_online_name']\n",
    "#     Noon_uae - For barcode comment before .apply(lambda p: int(p) if p == p else '')\n",
    "    file['Product Barcode']=file['item_barcode'].apply(lambda p: barcode(p) if p == p else '')\n",
    "    file['Flipkart ProductID']=file[' Flipkart_Internal_ID '].apply(lambda x: barcode(x) if x == x else '')\n",
    "    file['Category Level1']=file['Dept (Category_level_1)']\n",
    "    file['Category Level2']=file['Section (Category_level_2)']\n",
    "    file['Category Level3']=file['Family (Category_level_3)']\n",
    "    file['Category Level4']=file['Sub_Family(Category_level_4)']\n",
    "    #file['Category Level4']=file['Sub_Family(Category_level_4)']`a\n",
    "    del file[' Flipkart_product_online_name']\n",
    "    del file['Competitor']\n",
    "    del file['competitor_url']\n",
    "    del file['item_barcode']\n",
    "    del file[' Flipkart_Internal_ID ']\n",
    "    del file['input_B']\n",
    "    del file['Dept (Category_level_1)']\n",
    "    del file['Section (Category_level_2)']\n",
    "    del file['Family (Category_level_3)']\n",
    "    del file['Sub_Family(Category_level_4)']\n",
    "    del file['Competitor_Internal_product ID']\n",
    "    del file['Unnamed: 12']\n",
    "#    del file['Unnamed: 13']      #uncomment for KSA and Egypt and comment for UAE\n",
    "#    del file['Unnamed: 14']     #uncomment for KSA and comment for Egypt and UAE\n",
    "#     del file['Unnamed: 47']     #uncomment for KSA and comment for Egypt and UAE\n",
    "    finalOutput=file.to_csv(FinalPath+'\\\\Output\\\\'+filename+'.csv',index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
